# üìä L√≥gica de Ingesta de `salida.json` a Base de Datos Vectorial

## üéØ Objetivo
Explicar c√≥mo se procesa el archivo `salida.json` (lexic√≥n Bora-Espa√±ol) para poblar la base de datos Supabase con embeddings vectoriales para b√∫squeda sem√°ntica.

---

## üìÅ Archivo Fuente: `salida.json`

### Estructura Original del JSON
```json
[
  {
    "lemma": "a√°buk…®",
    "gloss_es": "sol",
    "pos": "s",
    "pos_full": "sustantivo",
    "page": 1,
    "variants": ["a√°buk…®…®"],
    "raw": "...",
    "examples": [
      {
        "bora": "A√°buk…® tsaall√©",
        "es": "El sol brilla"
      }
    ],
    "subentries": [
      {
        "sublemma": "a√°buk…® ts√∫√∫hne",
        "gloss_es": "mediod√≠a",
        "pos": "loc",
        "examples": [...]
      }
    ]
  },
  ...
]
```

---

## üîÑ Pipeline de Ingesta

### Script Principal: `ingest_bora_docs.py`

```bash
python backend/scripts/ingest_bora_docs.py \
  --path ../salida.json \
  --batch-size 400 \
  --embed-batch-size 64 \
  --reset
```

---

## üìä Tablas de Destino en Supabase

### 1. **`lexicon_lemmas`** (Palabras base)
- **Qu√© se guarda**: Cada palabra/lemma √∫nica del diccionario
- **Informaci√≥n extra√≠da**:
  ```python
  {
    'lemma': e.get('lemma'),              # Palabra en Bora (ej: "a√°buk…®")
    'gloss_es': e.get('gloss_es'),        # Definici√≥n en espa√±ol (ej: "sol")
    'pos': e.get('pos'),                   # Part of speech (ej: "s" = sustantivo)
    'pos_full': e.get('pos_full'),        # Categor√≠a completa (ej: "sustantivo")
    'page': e.get('page'),                 # P√°gina del diccionario original
    'variants': e.get('variants'),         # Variantes ortogr√°ficas
    'raw': e.get('raw'),                   # Texto crudo original
    'source': 'salida.json'
  }
  ```

### 2. **`lexicon_subentries`** (Subentradas/frases derivadas)
- **Qu√© se guarda**: Frases compuestas o acepciones adicionales
- **Informaci√≥n extra√≠da**:
  ```python
  {
    'lemma_id': lemma_id,                  # FK al lemma padre
    'sublemma': sub.get('sublemma'),       # Frase derivada (ej: "a√°buk…® ts√∫√∫hne")
    'gloss_es': sub.get('gloss_es'),       # Traducci√≥n (ej: "mediod√≠a")
    'pos': sub.get('pos'),                 # Categor√≠a gramatical
    'page': sub.get('page'),
    'variants': sub.get('variants'),
    'raw': sub.get('raw')
  }
  ```

### 3. **`lexicon_examples`** (Ejemplos de uso)
- **Qu√© se guarda**: Pares de oraciones Bora-Espa√±ol
- **Informaci√≥n extra√≠da**:
  ```python
  {
    'lemma_id': lemma_id,                  # FK al lemma
    'subentry_id': subentry_id,            # FK a subentrada (si aplica)
    'bora_text': ex.get('bora'),           # Oraci√≥n en Bora
    'spanish_text': ex.get('es'),          # Traducci√≥n al espa√±ol
    'category': pos_full or pos,           # Categor√≠a gramatical
    'page': page,
    'source': 'salida.json'
  }
  ```

### 4. **`bora_docs`** (Documentos para b√∫squeda vectorial)
- **Qu√© se guarda**: Textos estructurados para generar embeddings
- **3 tipos de documentos**:

#### **Tipo 1: LEMMA** (Definiciones de palabras)
```python
# Formato del texto:
"[LEMMA] {palabra} | DEF_ES: {definici√≥n} | POS: {categor√≠a} | PAG: {p√°gina}"

# Ejemplo real:
"[LEMMA] a√°buk…® | DEF_ES: sol | POS: sustantivo | PAG: 1"
```

#### **Tipo 2: SUBENTRY** (Frases compuestas)
```python
# Formato del texto:
"[SUBLEMMA] {frase} | DEF_ES: {definici√≥n} | POS: {categor√≠a} | PAG: {p√°gina}"

# Ejemplo real:
"[SUBLEMMA] a√°buk…® ts√∫√∫hne | DEF_ES: mediod√≠a | POS: locuci√≥n | PAG: 1"
```

#### **Tipo 3: EXAMPLE** (Oraciones completas)
```python
# Formato del texto:
"BORA: {oraci√≥n_bora} [SEP] ES: {oraci√≥n_espa√±ol} [SEP] LEMMA: {palabra_base} POS: {categor√≠a}"

# Ejemplo real:
"BORA: A√°buk…® tsaall√© [SEP] ES: El sol brilla [SEP] LEMMA: a√°buk…® POS: sustantivo"
```

---

## üßÆ Generaci√≥n de Embeddings Vectoriales

### Modelo Usado: OpenAI `text-embedding-3-small`
- **Dimensionalidad**: 1536 dimensiones
- **API**: OpenAI Embeddings API
- **Configuraci√≥n**: `USE_EMBEDDING_API=true` en `.env`

### Proceso de Vectorizaci√≥n

```python
# 1. Para cada documento en bora_docs, se genera su embedding:
texts = [
    "[LEMMA] a√°buk…® | DEF_ES: sol | POS: sustantivo | PAG: 1",
    "BORA: A√°buk…® tsaall√© [SEP] ES: El sol brilla [SEP] LEMMA: a√°buk…® POS: sustantivo",
    ...
]

# 2. Llamada a OpenAI en batches de 64:
embeddings = openai.embeddings.create(
    model="text-embedding-3-small",
    input=texts
)

# 3. Se guarda el vector de 1536 dimensiones:
{
  'doc_id': doc_id,
  'embedding_1536': [0.023, -0.145, 0.678, ...],  # 1536 floats
  'model': 'text-embedding-3-small',
  'created_at': datetime.now()
}
```

---

## üìã Informaci√≥n que S√ç se Almacena

### ‚úÖ **Datos Incluidos**:
1. **Lemmas (palabras base)**:
   - Palabra en Bora (`lemma`)
   - Definici√≥n en espa√±ol (`gloss_es`)
   - Categor√≠a gramatical (`pos`, `pos_full`)
   - P√°gina del diccionario (`page`)
   - Variantes ortogr√°ficas (`variants`)

2. **Subentries (frases derivadas)**:
   - Frase completa en Bora (`sublemma`)
   - Traducci√≥n al espa√±ol (`gloss_es`)
   - Categor√≠a gramatical
   - Relaci√≥n con lemma padre

3. **Examples (oraciones de ejemplo)**:
   - Oraci√≥n en Bora (`bora_text`)
   - Traducci√≥n al espa√±ol (`spanish_text`)
   - Relaci√≥n con lemma/subentry
   - Categor√≠a contextual

4. **Embeddings vectoriales** (para b√∫squeda sem√°ntica):
   - Vector de 1536 dimensiones para cada documento
   - Metadata completa del lemma/ejemplo

---

## ‚ùå Informaci√≥n que NO se Almacena / Se Omite

### üö´ **Datos Excluidos**:

1. **Campos sin valor (`None`, `""`, `[]`)**:
   ```python
   # Si gloss_es est√° vac√≠o, se descarta esa entrada:
   if not lemma or not gloss:
       continue  # ‚ùå No se guarda
   ```

2. **Duplicados exactos**:
   ```python
   # Se valida unicidad con claves compuestas:
   key = (lemma, 'salida.json')
   if key in seen:
       continue  # ‚ùå Ya existe, se omite
   ```

3. **Ejemplos sin par completo Bora-Espa√±ol**:
   ```python
   bora = ex.get('bora').strip()
   es = ex.get('es').strip()
   if not (bora and es):
       continue  # ‚ùå Si falta uno de los dos, se omite
   ```

4. **Sin√≥nimos** (fuera de alcance):
   ```python
   'synonyms': None  # ‚ùå Campo reservado pero no usado
   ```

5. **Informaci√≥n redundante en `raw`**:
   - Se guarda el campo `raw` completo
   - Pero NO se usa para embeddings (solo los campos estructurados)

6. **Metadata adicional del JSON original** que no est√© en el schema:
   - Solo se extraen los campos definidos expl√≠citamente
   - Campos custom del JSON se ignoran

---

## üìä Estad√≠sticas T√≠picas de Ingesta

Basado en un corpus promedio de `salida.json`:

```
üìÑ Entradas en JSON: 2,450
üìù Lemmas √∫nicos: 2,180
üìë Subentradas: 450
üí¨ Ejemplos totales: 3,200
üéØ Documentos con embeddings (bora_docs): ~5,830
   - Lemmas: 2,180
   - Subentries: 450
   - Examples: 3,200
```

---

## üîç B√∫squeda Sem√°ntica (C√≥mo se Usa)

### Query del Usuario:
```python
query = "¬øC√≥mo digo hola en Bora?"
```

### Proceso:
1. **Generar embedding de la query** (1536 dims con OpenAI):
   ```python
   query_embedding = openai.embeddings.create(
       model="text-embedding-3-small",
       input=query
   )
   ```

2. **Buscar en Supabase con pgvector**:
   ```sql
   SELECT 
     bd.content,
     bd.metadata,
     bd.embedding_1536 <=> query_vector AS distance
   FROM bora_docs bd
   WHERE bd.embedding_1536 IS NOT NULL
   ORDER BY bd.embedding_1536 <=> query_vector
   LIMIT 5;
   ```

3. **Resultados ordenados por similitud**:
   ```
   1. "[LEMMA] k√≥htsapa | DEF_ES: hola, buenos d√≠as | POS: interjecci√≥n"
   2. "BORA: K√≥htsapa, ¬øk√≥je tsaa? [SEP] ES: Hola, ¬øc√≥mo est√°s?"
   3. "[SUBLEMMA] k√≥htsapa √≠m…®…®n√© | DEF_ES: buenos d√≠as"
   ...
   ```

---

## üõ†Ô∏è Scripts de Mantenimiento

### Backfill de Embeddings (Migraci√≥n a 1536 dims)
```bash
# Para actualizar embeddings existentes a OpenAI 1536:
python backend/scripts/backfill_embeddings_1536.py \
  --target lexicon \
  --batch 64 \
  --limit 0
```

### Validaci√≥n de Integridad
```bash
# Verificar que todos los ejemplos tengan embeddings:
SELECT COUNT(*) 
FROM lexicon_examples le 
LEFT JOIN bora_docs bd ON bd.example_id = le.id
WHERE bd.id IS NULL;
```

---

## üìå Puntos Clave

1. **Deduplicaci√≥n**: Se evitan registros duplicados con claves compuestas
2. **Validaci√≥n**: Solo se guardan entradas con datos completos y v√°lidos
3. **Embeddings contextuales**: Cada tipo de documento tiene formato espec√≠fico
4. **B√∫squeda sem√°ntica**: Usa similitud coseno en espacio vectorial de 1536 dims
5. **Escalabilidad**: Procesamiento en batches (64-400 registros)
6. **Trazabilidad**: Campo `source` siempre registra origen (`salida.json`)

---

## üîó Referencias

- **Script principal**: `backend/scripts/ingest_bora_docs.py`
- **Backfill embeddings**: `backend/scripts/backfill_embeddings_1536.py`
- **Modelo embedding**: OpenAI `text-embedding-3-small` (1536 dims)
- **Vector DB**: Supabase con pgvector extension
- **SQL Schemas**: `docs/SETUP_SUPABASE.md`
