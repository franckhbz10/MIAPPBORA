# Preprocesamiento de Queries con LLM

## ðŸŽ¯ Objetivo

Mejorar la precisiÃ³n de la bÃºsqueda vectorial en el **Mentor Bora** eliminando el ruido conversacional de las queries de usuarios antes de generar los embeddings.

## âŒ Problema Identificado

### Antes del Preprocesamiento

Cuando un usuario hace una pregunta con contexto conversacional:

```
"hola soy pablito, estoy estudiando y no se como saludar en la lengua bora"
```

El sistema **vectorizaba la query completa**, incluyendo:
- Saludos ("hola")
- Presentaciones ("soy pablito")
- Contexto personal ("estoy estudiando")
- La pregunta real ("como saludar en la lengua bora")

**Problema**: El embedding resultante contenÃ­a demasiado ruido, reduciendo la precisiÃ³n de la bÃºsqueda semÃ¡ntica.

## âœ… SoluciÃ³n Implementada

### Pipeline de Preprocesamiento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. QUERY ORIGINAL (con ruido)                               â”‚
â”‚    "hola soy pablito, estoy estudiando y no se como         â”‚
â”‚     saludar en la lengua bora"                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. EXTRACCIÃ“N DE KEYWORDS (gpt-4o-mini)                     â”‚
â”‚    Prompt especializado extrae tÃ©rminos clave               â”‚
â”‚    Resultado: "saludar"                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. EMBEDDING (OpenAI text-embedding-3-small)                â”‚
â”‚    Solo vectoriza la query limpia                           â”‚
â”‚    Vector 1536 dims de "saludar"                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. BÃšSQUEDA VECTORIAL (pgvector)                            â”‚
â”‚    Busca top-k documentos similares a "saludar"             â”‚
â”‚    Mayor precisiÃ³n sin ruido conversacional                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. RESPUESTA FINAL (gpt-4o-mini)                            â”‚
â”‚    Usa QUERY ORIGINAL completa + contexto recuperado        â”‚
â”‚    Responde de forma conversacional y natural               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componente Clave: `_extract_search_keywords()`

UbicaciÃ³n: `backend/services/rag_service.py`

```python
async def _extract_search_keywords(self, query: str) -> str:
    """
    Extrae keywords/frases relevantes usando gpt-4o-mini.
    
    Ejemplos:
    - Input:  "hola soy pablito, estoy estudiando y no se como saludar en la lengua bora"
      Output: "saludar"
    
    - Input:  "oye amigo, necesito saber como se dice casa en bora"
      Output: "casa"
    
    - Input:  "que significa Ã¡Ã¡bukÉ¨ en espaÃ±ol"
      Output: "Ã¡Ã¡bukÉ¨"
    """
```

## ðŸ”§ ImplementaciÃ³n TÃ©cnica

### ConfiguraciÃ³n del Modelo

- **Modelo**: `gpt-4o-mini`
- **Temperatura**: `0.1` (baja para consistencia)
- **Max tokens**: `50` (keywords cortas)
- **Timeout**: Hereda de `settings.OPENAI_TIMEOUT`

### Prompt de ExtracciÃ³n

El prompt estÃ¡ diseÃ±ado con **few-shot learning** para entrenar al modelo:

```
Eres un asistente que extrae palabras o frases clave de consultas de traducciÃ³n.

Tu tarea: Identificar QUÃ‰ palabra o frase en espaÃ±ol el usuario quiere 
traducir al idioma Bora, ignorando todo el ruido conversacional.

Reglas:
1. Extrae SOLO la palabra/frase que necesita traducciÃ³n
2. Ignora saludos, presentaciones, contexto personal
3. Si hay mÃºltiples tÃ©rminos relacionados, mantÃ©n la frase completa
4. Responde ÃšNICAMENTE con la palabra/frase extraÃ­da, sin explicaciones

Ejemplos:
Usuario: "hola soy pablito, estoy estudiando y no se como saludar en la lengua bora"
Asistente: saludar

Usuario: "como digo yo soy estudiante en bora"
Asistente: yo soy estudiante
```

### ValidaciÃ³n y Fallbacks

```python
# ValidaciÃ³n bÃ¡sica
if not extracted or len(extracted) > len(query) * 1.5:
    logger.warning(f"Keyword extraction invÃ¡lida, usando query original")
    return query

# Fallback si OpenAI no disponible
if not self.openai_adapter:
    logger.warning("OpenAI adapter no disponible, usando query original")
    return query

# Fallback en caso de error
except Exception as e:
    logger.error(f"Error en keyword extraction: {e}")
    return query  # Siempre retorna la query original como fallback seguro
```

## ðŸ“Š Beneficios

### 1. **Mayor PrecisiÃ³n en BÃºsqueda Vectorial**

- âœ… Embeddings mÃ¡s enfocados en tÃ©rminos relevantes
- âœ… ReducciÃ³n de falsos positivos por ruido conversacional
- âœ… Mejora en ranking de resultados relevantes

### 2. **Respuestas MÃ¡s Naturales**

- âœ… Query original se mantiene para el LLM final
- âœ… El LLM puede responder con contexto conversacional
- âœ… No se pierde informaciÃ³n del usuario

### 3. **Manejo de Frases Complejas**

El sistema detecta y mantiene frases completas cuando es necesario:

```
Input:  "como se dice yo soy estudiante en bora"
Output: "yo soy estudiante"  â† Mantiene la frase completa
```

### 4. **DirecciÃ³n AgnÃ³stica**

El preprocesamiento funciona para ambas direcciones de traducciÃ³n:

- **EspaÃ±ol â†’ Bora**: `"como se dice casa"` â†’ `"casa"`
- **Bora â†’ EspaÃ±ol**: `"que significa Ã¡Ã¡bukÉ¨"` â†’ `"Ã¡Ã¡bukÉ¨"`

## ðŸ§ª Testing

### Script de Prueba

```bash
cd backend
python -m scripts.test_query_preprocessing
```

Prueba casos como:
- Queries con ruido conversacional
- Queries inversas (Bora â†’ EspaÃ±ol)
- Queries con frases complejas
- Queries cortas/directas

### Casos de Prueba

```python
TEST_QUERIES = [
    "hola soy pablito, estoy estudiando y no se como saludar en la lengua bora",
    "oye amigo, necesito saber como se dice casa en bora",
    "que significa Ã¡Ã¡bukÉ¨ en espaÃ±ol",
    "necesito ayuda para traducir la frase me gusta aprender idiomas",
    "casa",  # Query directa
]
```

## â±ï¸ Impacto en Performance

### Latencia Adicional

El preprocesamiento agrega una llamada a gpt-4o-mini:

```
Timings tÃ­picos:
- preprocessing_ms: ~200-400ms (llamada LLM)
- embedding_ms: ~100-200ms (OpenAI embeddings)
- vector_search_ms: ~50-100ms (pgvector)
- llm_ms: ~800-1500ms (respuesta final)

Total: ~1200-2200ms (preprocesamiento = ~15-20% del total)
```

### Tradeoff

- âŒ **Costo**: +200-400ms de latencia
- âœ… **Beneficio**: Mejora significativa en precisiÃ³n de resultados
- âœ… **ROI**: Vale la pena para mejor experiencia de usuario

## ðŸš€ Deploy

### Variables de Entorno Requeridas

```bash
# .env o Railway Variables
OPENAI_ENABLED=true
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=500
```

### VerificaciÃ³n Post-Deploy

```bash
# Railway Logs - buscar este patrÃ³n
ðŸ” Query preprocessing | Original: '...' â†’ Cleaned: '...'
```

## ðŸ“ Notas Importantes

### âš ï¸ No Interfiere con Respuesta Final

El preprocesamiento **solo afecta la recuperaciÃ³n** (bÃºsqueda vectorial). El LLM final siempre recibe la **query original completa** para poder:

- Entender el contexto conversacional
- Responder de forma natural
- Usar informaciÃ³n adicional del usuario si es relevante

### âš ï¸ Fallback Seguro

Si el preprocesamiento falla por cualquier razÃ³n:
- OpenAI no disponible
- Error de red
- Respuesta invÃ¡lida

El sistema **automÃ¡ticamente usa la query original** sin romper la funcionalidad.

### âš ï¸ CachÃ© No Afectado

El cachÃ© sigue usando la query original como key, asÃ­ que:
- Queries idÃ©nticas usan cachÃ©
- El preprocesamiento solo ocurre en cache misses

## ðŸ”® Mejoras Futuras

### 1. **Modelo Local Opcional**

Usar un modelo pequeÃ±o local (Phi-3, Qwen) para reducir latencia y costos:

```python
# Fallback a modelo local si OpenAI no disponible
if not self.openai_adapter:
    return self._extract_keywords_local(query)  # TODO
```

### 2. **CachÃ© de Keywords ExtraÃ­das**

Cachear keywords para queries frecuentes:

```python
_keyword_cache: Dict[str, str] = {}  # query -> cleaned_query
```

### 3. **Regex Patterns para Casos Simples**

Usar regex para casos triviales antes de llamar al LLM:

```python
# PatrÃ³n: "como se dice X"
match = re.search(r'como se dice (.+?)(?:\s+en\s+bora)?$', query)
if match:
    return match.group(1)  # Skip LLM call
```

## ðŸ“š Referencias

- **CÃ³digo**: `backend/services/rag_service.py` (lÃ­nea ~87)
- **Tests**: `backend/scripts/test_query_preprocessing.py`
- **Modelo**: OpenAI gpt-4o-mini
- **Prompting**: Few-shot learning con ejemplos
