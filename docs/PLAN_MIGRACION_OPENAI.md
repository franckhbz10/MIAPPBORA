# Plan de Trabajo: Migraci√≥n a OpenAI API

## üìã Objetivo
Eliminar el microservicio LLM local (que consume muchos recursos) y migrar a **OpenAI API** usando **GPT-4** o **GPT-3.5-turbo** para generar respuestas del mentor Bora.

---

## üéØ Ventajas de la Migraci√≥n

‚úÖ **Menor consumo de recursos:** No necesita cargar modelos locales (4GB+ RAM)  
‚úÖ **Mejor calidad:** GPT-4 es m√°s potente que Qwen3-1.7B  
‚úÖ **M√°s r√°pido:** Sin overhead de carga de modelo  
‚úÖ **Menor latencia:** API optimizada en la nube  
‚úÖ **Escalable:** Sin l√≠mites de hardware local  
‚úÖ **Mantenimiento:** Sin preocupaciones por actualizaciones de transformers/torch  

---

## üìù Fases del Plan

### ‚úÖ **Fase 1: Preparaci√≥n (10 min)**
**Objetivo:** Obtener API key de OpenAI y verificar acceso

**Tareas:**
1. ‚úÖ Crear cuenta en [OpenAI Platform](https://platform.openai.com/)
2. ‚úÖ Generar API key en [API Keys](https://platform.openai.com/api-keys)
3. ‚úÖ Verificar cr√©ditos disponibles (o configurar billing)
4. ‚úÖ Probar API key con un request simple

**Archivos a modificar:** Ninguno a√∫n

---

### ‚úÖ **Fase 2: Crear adaptador OpenAI (20 min)**
**Objetivo:** Implementar nuevo adaptador que reemplace al microservicio LLM

**Tareas:**
1. ‚úÖ Crear `backend/adapters/openai_adapter.py`
2. ‚úÖ Instalar `openai` library: `pip install openai`
3. ‚úÖ Implementar clase `OpenAIAdapter` con:
   - `chat_completion()` - Generar respuesta usando GPT-4/3.5-turbo
   - `generate_embedding()` - Usar OpenAI embeddings (opcional)
4. ‚úÖ A√±adir configuraci√≥n al `.env`:
   ```properties
   OPENAI_API_KEY=sk-...
   OPENAI_MODEL=gpt-4  # o gpt-3.5-turbo
   OPENAI_TEMPERATURE=0.7
   OPENAI_MAX_TOKENS=500
   ```
5. ‚úÖ Actualizar `backend/config/settings.py` con nuevas variables

**Archivos a crear/modificar:**
- `backend/adapters/openai_adapter.py` (NUEVO)
- `backend/.env` (a√±adir OPENAI_API_KEY)
- `backend/config/settings.py` (a√±adir settings de OpenAI)
- `backend/requirements.txt` (a√±adir openai)

---

### ‚úÖ **Fase 3: Modificar RAG Service (15 min)**
**Objetivo:** Actualizar `rag_service.py` para usar OpenAI en lugar del microservicio

**Tareas:**
1. ‚úÖ Modificar `backend/services/rag_service.py`:
   - Remover import de `llm_service_adapter`
   - A√±adir import de `openai_adapter`
   - Actualizar `__init__` para usar OpenAI adapter
   - Modificar `generate_response()` para llamar a OpenAI
   - Mantener fallback a HuggingFace local si OpenAI falla

**Flujo actualizado:**
```
1. Intentar OpenAI API (GPT-4/3.5-turbo)
   ‚îú‚îÄ √âxito ‚Üí Retornar respuesta
   ‚îî‚îÄ Fallo ‚Üí Log warning + continuar con fallback

2. Fallback a HuggingFace local (Qwen3-1.7B)
   ‚îú‚îÄ √âxito ‚Üí Retornar respuesta
   ‚îî‚îÄ Fallo ‚Üí Respuesta b√°sica desde contexto
```

**Archivos a modificar:**
- `backend/services/rag_service.py`

---

### ‚úÖ **Fase 4: Actualizar configuraci√≥n (10 min)**
**Objetivo:** Deshabilitar microservicio LLM y configurar OpenAI

**Tareas:**
1. ‚úÖ Modificar `backend/.env`:
   ```properties
   # Deshabilitar microservicio LLM
   LLM_SERVICE_ENABLED=false
   
   # Habilitar OpenAI
   OPENAI_ENABLED=true
   OPENAI_API_KEY=sk-...
   OPENAI_MODEL=gpt-4
   ```

2. ‚úÖ Actualizar `backend/config/settings.py`:
   ```python
   OPENAI_ENABLED: bool = False
   OPENAI_API_KEY: Optional[str] = None
   OPENAI_MODEL: str = "gpt-4"  # o "gpt-3.5-turbo"
   OPENAI_TEMPERATURE: float = 0.7
   OPENAI_MAX_TOKENS: int = 500
   ```

3. ‚úÖ Modificar `backend/main.py` (startup):
   - Remover health check del microservicio LLM
   - A√±adir verificaci√≥n de OpenAI API key

**Archivos a modificar:**
- `backend/.env`
- `backend/config/settings.py`
- `backend/main.py`

---

### ‚úÖ **Fase 5: Testing (15 min)**
**Objetivo:** Validar que OpenAI funciona correctamente

**Tareas:**
1. ‚úÖ Crear `backend/test_openai.py`:
   ```python
   # Test simple de OpenAI API
   from adapters.openai_adapter import get_openai_adapter
   
   adapter = get_openai_adapter()
   response = adapter.chat_completion([
       {"role": "user", "content": "¬øQu√© es Bora?"}
   ])
   print(response)
   ```

2. ‚úÖ Arrancar backend: `uvicorn main:app --reload`
3. ‚úÖ Probar endpoint del mentor: `POST /api/rag/query`
4. ‚úÖ Verificar logs que muestran uso de OpenAI
5. ‚úÖ Probar fallback desactivando OpenAI temporalmente

**Archivos a crear:**
- `backend/test_openai.py` (temporal, para validar)

---

### ‚úÖ **Fase 6: Limpieza (10 min)**
**Objetivo:** Eliminar microservicio LLM y archivos innecesarios

**Tareas:**
1. ‚úÖ Detener proceso del microservicio LLM (puerto 8001)
2. ‚úÖ Eliminar carpeta completa: `llm_service/`
3. ‚úÖ Remover archivos de adaptador LLM:
   - `backend/adapters/llm_service_adapter.py`
4. ‚úÖ Limpiar imports innecesarios en el c√≥digo
5. ‚úÖ Actualizar `.gitignore` si es necesario

**Comandos:**
```powershell
# Detener microservicio (si est√° corriendo)
Get-Process -Name python | Where-Object {$_.Path -like "*llm_service*"} | Stop-Process

# Eliminar carpeta
Remove-Item -Recurse -Force llm_service

# Eliminar adaptador viejo
Remove-Item backend/adapters/llm_service_adapter.py
```

**Archivos a eliminar:**
- `llm_service/` (carpeta completa)
- `backend/adapters/llm_service_adapter.py`

---

### ‚úÖ **Fase 7: Documentaci√≥n (10 min)**
**Objetivo:** Actualizar documentaci√≥n del proyecto

**Tareas:**
1. ‚úÖ Actualizar `README.md`:
   - Indicar que usa OpenAI API
   - Explicar c√≥mo obtener API key
   - Documentar variables de entorno

2. ‚úÖ Crear `docs/SETUP_OPENAI.md`:
   - Gu√≠a paso a paso para configurar OpenAI
   - Estimaci√≥n de costos
   - Modelos disponibles (GPT-4, GPT-3.5-turbo)

3. ‚úÖ Actualizar `docs/GUIA_INSTALACION.md`:
   - Remover secci√≥n de microservicio LLM
   - A√±adir secci√≥n de OpenAI

4. ‚úÖ Archivar documentaci√≥n vieja:
   - Mover `docs/LLM_MICROSERVICE.md` a `docs/archive/`
   - Mover `docs/PLAN_IMPLEMENTACION_LLM.md` a `docs/archive/`

**Archivos a modificar/crear:**
- `README.md`
- `docs/SETUP_OPENAI.md` (NUEVO)
- `docs/GUIA_INSTALACION.md`
- `docs/archive/` (carpeta para docs viejos)

---

### ‚úÖ **Fase 8: Optimizaci√≥n (Opcional - 15 min)**
**Objetivo:** Optimizar uso de OpenAI para reducir costos

**Tareas:**
1. ‚è≥ Implementar cach√© de respuestas frecuentes
2. ‚è≥ Usar `gpt-3.5-turbo` para consultas simples
3. ‚è≥ Implementar rate limiting
4. ‚è≥ A√±adir m√©tricas de uso (tokens consumidos)
5. ‚è≥ Configurar timeout adecuado

**Archivos a modificar:**
- `backend/adapters/openai_adapter.py`
- `backend/services/rag_service.py`

---

## ‚è±Ô∏è Cronograma Estimado

| Fase | Duraci√≥n | Acumulado |
|------|----------|-----------|
| 1. Preparaci√≥n | 10 min | 0:10 |
| 2. Crear adaptador OpenAI | 20 min | 0:30 |
| 3. Modificar RAG Service | 15 min | 0:45 |
| 4. Actualizar configuraci√≥n | 10 min | 0:55 |
| 5. Testing | 15 min | 1:10 |
| 6. Limpieza | 10 min | 1:20 |
| 7. Documentaci√≥n | 10 min | 1:30 |
| 8. Optimizaci√≥n (opcional) | 15 min | 1:45 |

**Tiempo total estimado: 1.5 - 2 horas**

---

## üí∞ Estimaci√≥n de Costos OpenAI

### Modelos Recomendados

#### GPT-4 Turbo (Recomendado para producci√≥n)
- **Input:** $0.01 / 1K tokens
- **Output:** $0.03 / 1K tokens
- **Contexto:** 128K tokens
- **Calidad:** Excelente

#### GPT-3.5 Turbo (Recomendado para desarrollo)
- **Input:** $0.0005 / 1K tokens  
- **Output:** $0.0015 / 1K tokens
- **Contexto:** 16K tokens
- **Calidad:** Muy buena

### Ejemplo de Uso
**Supuestos:**
- 100 consultas/d√≠a
- ~500 tokens promedio por consulta (input + output)

**Costo mensual (30 d√≠as):**
- GPT-4 Turbo: ~$45/mes
- GPT-3.5 Turbo: ~$2.25/mes

**Recomendaci√≥n:** Comenzar con **GPT-3.5-turbo** para desarrollo y testing, migrar a **GPT-4** si necesitas mejor calidad.

---

## üîß C√≥digo de Ejemplo

### OpenAI Adapter
```python
# backend/adapters/openai_adapter.py
import openai
from config.settings import settings
import logging

logger = logging.getLogger(__name__)

class OpenAIAdapter:
    def __init__(self):
        self.api_key = settings.OPENAI_API_KEY
        self.model = settings.OPENAI_MODEL
        self.enabled = settings.OPENAI_ENABLED
        openai.api_key = self.api_key
        logger.info(f"OpenAIAdapter inicializado: {self.model}")
    
    def chat_completion(self, messages, max_tokens=None, temperature=None):
        if not self.enabled or not self.api_key:
            raise Exception("OpenAI no configurado")
        
        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=messages,
                max_tokens=max_tokens or settings.OPENAI_MAX_TOKENS,
                temperature=temperature or settings.OPENAI_TEMPERATURE
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error llamando a OpenAI: {e}")
            raise

_openai_adapter = None

def get_openai_adapter():
    global _openai_adapter
    if _openai_adapter is None:
        _openai_adapter = OpenAIAdapter()
    return _openai_adapter
```

### RAG Service (actualizado)
```python
# backend/services/rag_service.py
from adapters.openai_adapter import get_openai_adapter

class RAGService:
    def __init__(self):
        self.openai_adapter = get_openai_adapter() if settings.OPENAI_ENABLED else None
        self.hf_adapter = get_huggingface_adapter()  # Fallback
        # ...
    
    async def generate_response(self, query, context, conversation_history=None):
        messages = self._build_messages(query, context, conversation_history)
        
        # Intentar OpenAI primero
        if self.openai_adapter:
            try:
                logger.info("ü§ñ Generando respuesta con OpenAI...")
                response = self.openai_adapter.chat_completion(messages)
                logger.info("‚úì Respuesta generada con OpenAI")
                return self._post_process_mentor_response(response)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è OpenAI fall√≥: {e}, usando fallback local")
        
        # Fallback a HuggingFace local
        logger.info("üîß Generando respuesta con HuggingFace local...")
        response = self.hf_adapter.chat_completion(messages)
        return self._post_process_mentor_response(response)
```

---

## üéØ Resultado Final

### Antes (Microservicio LLM)
```
Usuario ‚Üí Backend ‚Üí Microservicio:8001 ‚Üí Qwen3-1.7B ‚Üí Respuesta
          (8000)    (HTTP)              (4GB RAM)
```

### Despu√©s (OpenAI API)
```
Usuario ‚Üí Backend ‚Üí OpenAI API ‚Üí GPT-4 ‚Üí Respuesta
          (8000)    (HTTPS)     (Cloud)
```

### Beneficios
- ‚úÖ **-4GB RAM** (no carga modelo local)
- ‚úÖ **-1 servicio** (elimina microservicio)
- ‚úÖ **+Calidad** (GPT-4 > Qwen3-1.7B)
- ‚úÖ **+Velocidad** (API optimizada)
- ‚úÖ **+Estabilidad** (sin problemas de device_map, torch, etc.)

---

## ‚úÖ Checklist de Ejecuci√≥n

### Preparaci√≥n
- [ ] Crear cuenta en OpenAI
- [ ] Obtener API key
- [ ] Verificar cr√©ditos/billing
- [ ] Probar API key con curl

### Implementaci√≥n
- [ ] Instalar `openai` library
- [ ] Crear `openai_adapter.py`
- [ ] A√±adir variables al `.env`
- [ ] Actualizar `settings.py`
- [ ] Modificar `rag_service.py`
- [ ] Actualizar `main.py` (startup)

### Testing
- [ ] Probar OpenAI adapter aislado
- [ ] Arrancar backend y probar endpoint
- [ ] Verificar logs de uso de OpenAI
- [ ] Probar fallback

### Limpieza
- [ ] Detener microservicio LLM
- [ ] Eliminar carpeta `llm_service/`
- [ ] Eliminar `llm_service_adapter.py`
- [ ] Limpiar imports

### Documentaci√≥n
- [ ] Actualizar README.md
- [ ] Crear SETUP_OPENAI.md
- [ ] Actualizar GUIA_INSTALACION.md
- [ ] Archivar docs viejos

---

## üöÄ Comando de Inicio

```powershell
# Comenzar con Fase 1
# 1. Ir a https://platform.openai.com/api-keys
# 2. Crear API key
# 3. Ejecutar: pip install openai
# 4. Crear backend/adapters/openai_adapter.py
```

---

**¬øListo para comenzar con la Fase 1?**
